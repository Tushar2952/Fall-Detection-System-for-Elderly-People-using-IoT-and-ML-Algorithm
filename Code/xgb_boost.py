# -*- coding: utf-8 -*-
"""XGB Boost.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V_juopr_eN-W8Gpzo3iCc2jJL25iewuU
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from xgboost import XGBClassifier
from sklearn.metrics import (classification_report, confusion_matrix,
                            roc_curve, auc, precision_recall_curve,
                            average_precision_score, f1_score) # Import f1_score
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
import warnings
warnings.filterwarnings('ignore')
!pip install plotly
import plotly.graph_objs as go

# Set random seed for reproducibility
np.random.seed(42)

# --------------------------------------------------
# Synthetic Data Generation
# --------------------------------------------------
def generate_synthetic_data():
    """Generate balanced synthetic fall detection data"""
    np.random.seed(42)
    n_samples = 1000  # Reduced from original for faster execution

    # Features similar to real fall detection data
    features = {
        'AccMag': np.concatenate([
            np.random.normal(1.0, 0.2, n_samples//2),  # Normal movement
            np.random.normal(3.5, 1.0, n_samples//2)    # Fall movement
        ]),
        'JerkMag': np.concatenate([
            np.random.normal(0.5, 0.1, n_samples//2),  # Normal
            np.random.normal(2.5, 0.8, n_samples//2)   # Fall
        ]),
        'GyroMag': np.concatenate([
            np.random.normal(0.8, 0.2, n_samples//2),  # Normal
            np.random.normal(3.0, 1.0, n_samples//2)   # Fall
        ]),
        'Pressure': np.random.normal(101325, 100, n_samples),
        'Altitude': np.random.normal(50, 20, n_samples)
    }

    df = pd.DataFrame(features)
    df['Label'] = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
    df['Label'] = df['Label'].map({0: 'Fall Not Detected', 1: 'Fall Detected'})
    return df

# Generate and show data sample
df = generate_synthetic_data()
print("\nSample of Generated Data:")
print(df.head())

# --------------------------------------------------
# Data Preprocessing
# --------------------------------------------------
X = df.drop('Label', axis=1)
y = df['Label']
y_binary = y.map({'Fall Not Detected': 0, 'Fall Detected': 1})

# Split data (stratified to maintain class balance)
X_train, X_test, y_train, y_test, y_train_bin, y_test_bin = train_test_split(
    X, y, y_binary, test_size=0.3, stratify=y_binary, random_state=42)

# Check class distribution
print("\nClass Distribution:")
print(y.value_counts())

# --------------------------------------------------
# Quick Data Visualization
# --------------------------------------------------
def quick_visualizations(df):
    """Generate essential visualizations quickly"""
    plt.figure(figsize=(15, 10))

    # Feature distributions
    plt.subplot(2, 2, 1)
    sns.boxplot(x='Label', y='AccMag', data=df)
    plt.title('Acceleration Magnitude by Class')

    plt.subplot(2, 2, 2)
    sns.boxplot(x='Label', y='JerkMag', data=df)
    plt.title('Jerk Magnitude by Class')

    plt.subplot(2, 2, 3)
    sns.boxplot(x='Label', y='GyroMag', data=df)
    plt.title('Gyroscope Magnitude by Class')

    # Correlation matrix
    plt.subplot(2, 2, 4)
    sns.heatmap(df.corr(numeric_only=True), annot=True, cmap='coolwarm', center=0)
    plt.title('Feature Correlation Matrix')

    plt.tight_layout()
    plt.show()

quick_visualizations(df)

# --------------------------------------------------
# Optimized XGBoost Pipeline
# --------------------------------------------------
# Create pipeline with SMOTE for handling imbalance
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('feature_selection', SelectKBest(score_func=f_classif)),
    ('classifier', XGBClassifier(
        random_state=42,
        eval_metric='logloss',
        n_estimators=100
    ))
])

# Focused hyperparameter grid
param_grid = {
    'classifier__max_depth': [3, 5],
    'classifier__learning_rate': [0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'classifier__early_stopping_rounds': [10],
    'feature_selection__k': [3, 'all']
}

# Create grid search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=1
)
# Prepare the eval_set by transforming the test data through the pipeline steps
# Remove this entire block:
# X_test_transformed = X_test.copy()
# for step_name, step in pipeline.steps[:-1]:
#     if hasattr(step, 'transform'):
#         X_test_transformed = step.transform(X_test_transformed)
#     elif hasattr(step, 'fit_resample'):  # Skip SMOTE for test data
#         continue

# eval_set = [(X_test_transformed, y_test_bin)]

# Replace with:
eval_set = [(X_test, y_test_bin)] # Pass the original X_test and y_test_bin
# Remove early_stopping_rounds from param_grid
param_grid = {
    'classifier__max_depth': [3, 5],
    'classifier__learning_rate': [0.1, 0.2],
    'classifier__subsample': [0.8, 1.0],
    'feature_selection__k': [3, 'all']
}

# Create grid search
grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

print("\nStarting Grid Search...")
grid_search.fit(X_train, y_train_bin)

# ... (rest of the code) ...


# Get best model
best_model = grid_search.best_estimator_
print("\nBest Parameters:", grid_search.best_params_)

# --------------------------------------------------
# Essential Model Evaluation
# --------------------------------------------------
def evaluate_model(model, X_test, y_test, y_test_bin):
    """Perform essential model evaluation with visualizations"""
    # Predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]
    y_pred_labels = pd.Series(y_pred).map({0: 'Fall Not Detected', 1: 'Fall Detected'})

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred_labels))

    # Confusion Matrix
    plt.figure(figsize=(6, 6))
    sns.heatmap(confusion_matrix(y_test, y_pred_labels),
                annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Fall', 'Fall'],
                yticklabels=['No Fall', 'Fall'])
    plt.title('Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

    # ROC Curve
    fpr, tpr, _ = roc_curve(y_test_bin, y_proba)
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(6, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('ROC Curve')
    plt.legend(loc="lower right")
    plt.show()

    # Feature Importance
    if hasattr(model.named_steps['classifier'], 'feature_importances_'):
        importances = model.named_steps['classifier'].feature_importances_
        features = X.columns[model.named_steps['feature_selection'].get_support()]

        importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)

        plt.figure(figsize=(8, 4))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title('XGBoost Feature Importance')
        plt.show()

print("\nEvaluating Best Model...")
evaluate_model(best_model, X_test, y_test, y_test_bin)

# --------------------------------------------------
# Threshold Analysis
# --------------------------------------------------
def threshold_analysis(model, X_test, y_test_bin):
    """Analyze performance across different decision thresholds"""
    y_proba = model.predict_proba(X_test)[:, 1]

    thresholds = np.linspace(0, 1, 20)
    metrics = {
        'f1': [],
        'precision': [],
        'recall': []
    }

    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        report = classification_report(y_test_bin, y_pred, output_dict=True)
        metrics['f1'].append(report['1']['f1-score'])
        metrics['precision'].append(report['1']['precision'])
        metrics['recall'].append(report['1']['recall'])

    plt.figure(figsize=(10, 5))
    plt.plot(thresholds, metrics['f1'], label='F1 Score')
    plt.plot(thresholds, metrics['precision'], label='Precision')
    plt.plot(thresholds, metrics['recall'], label='Recall')

    # Mark default 0.5 threshold
    plt.axvline(x=0.5, color='gray', linestyle='--', label='Default Threshold (0.5)')

    # Find and mark optimal threshold (max F1)
    optimal_idx = np.argmax(metrics['f1'])
    optimal_thresh = thresholds[optimal_idx]
    plt.axvline(x=optimal_thresh, color='red', linestyle=':',
                label=f'Optimal Threshold ({optimal_thresh:.2f})')

    plt.title('Performance Metrics vs. Decision Threshold')
    plt.xlabel('Threshold')
    plt.ylabel('Score')
    plt.legend()
    plt.grid()
    plt.show()

print("\nPerforming Threshold Analysis...")
threshold_analysis(best_model, X_test, y_test_bin)

# --------------------------------------------------
# Final Model Summary
# --------------------------------------------------
print("\n=== MODEL TRAINING COMPLETE ===")
print(f"Best F1 Score: {grid_search.best_score_:.4f}")
print("Best Parameters:")
for param, value in grid_search.best_params_.items():
    print(f"- {param}: {value}")

    # --------------------------------------------------
# XGBoost Predictive Probability and Threshold Analysis
# --------------------------------------------------

# 1. Get predicted probabilities for the positive class ('Fall Detected')
y_proba = best_model.predict_proba(X_test)[:, 1]

# 2. Probability Distribution Plot
plt.figure(figsize=(12, 6))
sns.histplot(x=y_proba, hue=y_test, bins=30, kde=True,
             palette={'Fall Not Detected':'skyblue', 'Fall Detected':'coral'},
             alpha=0.6, element='step')
plt.title('XGBoost - Predicted Probability Distribution', fontsize=14)
plt.xlabel('Predicted Probability of Fall', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.axvline(x=0.5, color='gray', linestyle='--', label='Default Threshold (0.5)')
plt.legend(title='Actual Class')
plt.grid(alpha=0.3)
plt.show()
# 3. Threshold Optimization Analysis
thresholds = np.linspace(0, 1, 50)
metrics = {
    'f1': [],
    'precision': [],
    'recall': [],
    'fpr': []  # False Positive Rate
}

for thresh in thresholds:
    y_pred = (y_proba >= thresh).astype(int)
    # Map predicted labels to original labels using the dictionary
    y_pred_labels = pd.Series(y_pred).map({0: 'Fall Not Detected', 1: 'Fall Detected'})

    # Calculate metrics
    report = classification_report(y_test, y_pred_labels, output_dict=True)
    cm = confusion_matrix(y_test, y_pred_labels)
    tn, fp, fn, tp = cm.ravel()

    metrics['f1'].append(report['Fall Detected']['f1-score'])
    metrics['precision'].append(report['Fall Detected']['precision'])
    metrics['recall'].append(report['Fall Detected']['recall'])
    metrics['fpr'].append(fp / (fp + tn))

# Find optimal threshold (max F1 score)
optimal_idx = np.argmax(metrics['f1'])
optimal_thresh = thresholds[optimal_idx]

# 4. Interactive Threshold Analysis Plot (Plotly)
fig = go.Figure()

# Add metrics traces
fig.add_trace(go.Scatter(
    x=thresholds, y=metrics['f1'],
    mode='lines',
    name='F1 Score',
    line=dict(color='green', width=2)
))

fig.add_trace(go.Scatter(
    x=thresholds, y=metrics['precision'],
    mode='lines',
    name='Precision',
    line=dict(color='blue', width=2)
))

fig.add_trace(go.Scatter(
    x=thresholds, y=metrics['recall'],
    mode='lines',
    name='Recall',
    line=dict(color='red', width=2)
))

fig.add_trace(go.Scatter(
    x=thresholds, y=metrics['fpr'],
    mode='lines',
    name='False Positive Rate',
    line=dict(color='purple', width=2)
))

# Add optimal threshold line
fig.add_vline(
    x=optimal_thresh,
    line_dash="dash",
    line_color="black",
    annotation_text=f"Optimal Threshold: {optimal_thresh:.2f}",
    annotation_position="top right"
)

# Add default threshold line
fig.add_vline(
    x=0.5,
    line_dash="dot",
    line_color="gray",
    annotation_text="Default 0.5",
    annotation_position="bottom right"
)

fig.update_layout(
    title='XGBoost Threshold Optimization Analysis',
    xaxis_title='Decision Threshold',
    yaxis_title='Score/Rate',
    hovermode='x unified',
    width=900,
    height=600,
    legend=dict(orientation='h', yanchor='bottom', y=1.02, xanchor='right', x=1)
)

fig.show()

# 5. Metrics at Optimal Threshold
y_pred_optimal = (y_proba >= optimal_thresh).astype(int)
# Map predicted labels back to original labels (strings)
y_pred_labels_optimal = pd.Series(y_pred_optimal).map({0: 'Fall Not Detected', 1: 'Fall Detected'})

print(f"\n\033[1mPerformance at Optimal Threshold ({optimal_thresh:.2f}):\033[0m")
print(classification_report(y_test, y_pred_labels_optimal))

# 6. Confusion Matrix at Optimal Threshold
plt.figure(figsize=(6, 6))
cm_optimal = confusion_matrix(y_test, y_pred_labels_optimal)
sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_model.classes_,
            yticklabels=best_model.classes_)
plt.title(f'Confusion Matrix @ Threshold={optimal_thresh:.2f}')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()
# 6. Confusion Matrix at Optimal Threshold
plt.figure(figsize=(6, 6))
cm_optimal = confusion_matrix(y_test, y_pred_labels_optimal)
sns.heatmap(cm_optimal, annot=True, fmt='d', cmap='Blues',
            xticklabels=best_model.classes_,
            yticklabels=best_model.classes_)
plt.title(f'Confusion Matrix @ Threshold={optimal_thresh:.2f}')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

# 7. Probability Calibration Plot (Reliability Diagram)
from sklearn.calibration import calibration_curve

prob_true, prob_pred = calibration_curve(y_test_bin, y_proba, n_bins=10)

plt.figure(figsize=(8, 6))
plt.plot(prob_pred, prob_true, marker='o', label='XGBoost')
plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfectly calibrated')
plt.title('Probability Calibration Curve', fontsize=14)
plt.xlabel('Mean Predicted Probability')
plt.ylabel('Fraction of Positives')
plt.legend()
plt.grid(alpha=0.3)
plt.show()

# --------------------------------------------------
# XGBoost Probability and Threshold Analysis (Essential Only)
# --------------------------------------------------

# 1. Get predicted probabilities
y_proba = best_model.predict_proba(X_test)[:, 1]  # Probabilities for "Fall Detected"

# 2. Probability Distribution
plt.figure(figsize=(10,5))
sns.kdeplot(x=y_proba, hue=y_test, fill=True, palette={'Fall Not Detected':'blue', 'Fall Detected':'red'})
plt.title('Probability Distribution by True Class')
plt.xlabel('Predicted Probability of Fall')
plt.axvline(0.5, color='black', linestyle='--')
plt.show()

# 3. Find Optimal Threshold
thresholds = np.linspace(0, 1, 100)
# Convert y_proba to predicted labels before comparing to threshold:
y_pred_bin = best_model.predict(X_test) # Get binary predictions (0 or 1)
f1_scores = [f1_score(y_test_bin, y_pred_bin >= t, pos_label=1) for t in thresholds]
optimal_threshold = thresholds[np.argmax(f1_scores)]

# ... (rest of the code remains the same)
# 4. Threshold Analysis Plot
plt.figure(figsize=(10,5))
plt.plot(thresholds, f1_scores, label='F1 Score')
plt.axvline(optimal_threshold, color='red', linestyle=':',
           label=f'Optimal Threshold ({optimal_threshold:.2f})')
plt.title('F1 Score by Decision Threshold')
plt.xlabel('Threshold')
plt.ylabel('F1 Score')
plt.legend()
plt.show()

# 5. Metrics at Optimal Threshold
print(f"\nOptimal Threshold: {optimal_threshold:.2f}")
y_pred_optimal = (y_proba >= optimal_threshold).astype(int)  # Convert to 0/1
y_pred_labels_optimal = pd.Series(y_pred_optimal).map({0: 'Fall Not Detected', 1: 'Fall Detected'})  # Map to original labels
print(classification_report(y_test, y_pred_labels_optimal,
                          target_names=['Fall Not Detected', 'Fall Detected']))