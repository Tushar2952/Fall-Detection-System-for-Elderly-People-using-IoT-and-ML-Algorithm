# -*- coding: utf-8 -*-
"""Random Forest Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14zf7Vi4HJqOUoxQRN_oLUb_mnzxjv3F-
"""

# Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (classification_report, confusion_matrix, roc_curve, auc, precision_recall_curve,average_precision_score)
from sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay, roc_auc_score # Import necessary classes
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.feature_selection import SelectKBest, f_classif
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots

# Set random seed for reproducibility
np.random.seed(42)

# Load your dataset (replace with your actual data loading)
# df = pd.read_excel('merged_feature_data.xlsx')
# For this example, I'll create a synthetic dataset since we only saw "Fall Not Detected" samples

# --------------------------------------------------
# Synthetic Data Generation (Replace with your actual data)
# --------------------------------------------------
def generate_synthetic_data():
    # Create balanced synthetic data
    np.random.seed(42)
    n_samples = 1000

    # Features similar to your dataset
    features = {
        'AccMag': np.concatenate([np.random.normal(1.0, 0.2, n_samples//2),
                                np.random.normal(3.5, 1.0, n_samples//2)]),
        'JerkMag': np.concatenate([np.random.normal(0.5, 0.1, n_samples//2),
                                  np.random.normal(2.5, 0.8, n_samples//2)]),
        'GyroMag': np.concatenate([np.random.normal(0.8, 0.2, n_samples//2),
                                  np.random.normal(3.0, 1.0, n_samples//2)]),
        'Pressure': np.random.normal(101325, 100, n_samples),
        'Altitude': np.random.normal(50, 20, n_samples)
    }

    df = pd.DataFrame(features)
    df['Label'] = np.concatenate([np.zeros(n_samples//2), np.ones(n_samples//2)])
    df['Label'] = df['Label'].map({0: 'Fall Not Detected', 1: 'Fall Detected'})

    return df

df = generate_synthetic_data()
# --------------------------------------------------

# Data Preprocessing
X = df.drop('Label', axis=1)
y = df['Label']

# Split data (stratified to maintain class balance)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, stratify=y, random_state=42)

# Check class distribution
print("Class Distribution:")
print(y.value_counts())

# --------------------------------------------------
# 1. Feature Visualization
# --------------------------------------------------
def plot_feature_distributions(df):
    num_features = len(df.columns) - 1  # Exclude label
    n_cols = 3
    n_rows = (num_features + n_cols - 1) // n_cols

    plt.figure(figsize=(15, 5*n_rows))
    for i, col in enumerate(df.columns[:-1]):  # Exclude label
        plt.subplot(n_rows, n_cols, i+1)
        sns.histplot(data=df, x=col, hue='Label', kde=True, element='step')
        plt.title(f'Distribution of {col}')
    plt.tight_layout()
    plt.show()

plot_feature_distributions(df)

# Correlation Matrix
plt.figure(figsize=(10, 8))
corr = df.corr(numeric_only=True)
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Feature Correlation Matrix')
plt.show()

# --------------------------------------------------
# 2. Model Pipeline with SMOTE and Feature Selection
# --------------------------------------------------
# Create preprocessing and modeling pipeline
pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('feature_selection', SelectKBest(score_func=f_classif, k='all')),
    ('classifier', RandomForestClassifier(random_state=42, class_weight='balanced'))
])

# --------------------------------------------------
# 3. Hyperparameter Tuning with GridSearchCV
# --------------------------------------------------
param_grid = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 10, 20],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2],
    'feature_selection__k': [3, 5, 'all']
}

cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
grid_search = GridSearchCV(
    estimator=pipeline,
    param_grid=param_grid,
    cv=cv,
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

grid_search.fit(X_train, y_train)

# Best model
best_model = grid_search.best_estimator_
print("\nBest Parameters:", grid_search.best_params_)

# --------------------------------------------------
# 4. Model Evaluation with Visualizations
# --------------------------------------------------
def evaluate_model(model, X_test, y_test):
    # Predictions
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for positive class

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_test, y_pred))

    # Confusion Matrix
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(6, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=model.classes_, yticklabels=model.classes_)
    plt.title('Confusion Matrix')
    plt.ylabel('Actual')
    plt.xlabel('Predicted')
    plt.show()

    # ROC Curve
    fpr, tpr, thresholds = roc_curve(y_test, y_proba, pos_label='Fall Detected')
    roc_auc = auc(fpr, tpr)

    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    plt.legend(loc="lower right")
    plt.show()

    # Precision-Recall Curve
    precision, recall, _ = precision_recall_curve(
        y_test, y_proba, pos_label='Fall Detected')
    avg_precision = average_precision_score(
        y_test, y_proba, pos_label='Fall Detected')

    plt.figure(figsize=(8, 6))
    plt.plot(recall, precision, color='blue', lw=2,
             label=f'Precision-Recall (AP = {avg_precision:.2f})')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    plt.show()

    # Feature Importance (if using RandomForest)
    if hasattr(model.named_steps['classifier'], 'feature_importances_'):
        feature_importances = model.named_steps['classifier'].feature_importances_
        selected_features = model.named_steps['feature_selection'].get_support()
        features = X.columns[selected_features]

        importance_df = pd.DataFrame({
            'Feature': features,
            'Importance': feature_importances
        }).sort_values('Importance', ascending=False)

        plt.figure(figsize=(10, 6))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title('Feature Importance')
        plt.tight_layout()
        plt.show()

        # Interactive feature importance plot
        fig = px.bar(importance_df, x='Importance', y='Feature',
                     orientation='h', title='Feature Importance')
        fig.show()

# Evaluate best model
evaluate_model(best_model, X_test, y_test)

# --------------------------------------------------
# 5. Interactive Visualizations with Plotly
# --------------------------------------------------
def plot_interactive_distributions(df):
    fig = make_subplots(rows=2, cols=3, subplot_titles=df.columns[:-1])

    for i, col in enumerate(df.columns[:-1]):
        row = (i // 3) + 1
        col_num = (i % 3) + 1

        for label in df['Label'].unique():
            fig.add_trace(
                go.Histogram(
                    x=df[df['Label'] == label][col],
                    name=label,
                    opacity=0.75,
                    legendgroup=label,
                    showlegend=(i == 0)  # Only show legend for first plot
                ),
                row=row, col=col_num
            )

    fig.update_layout(
        title_text="Feature Distributions by Class",
        height=800,
        width=1200,
        barmode='overlay'
    )
    fig.show()

plot_interactive_distributions(df)

# Interactive correlation matrix
corr = df.corr(numeric_only=True)
fig = go.Figure(data=go.Heatmap(
    z=corr.values,
    x=corr.columns,
    y=corr.index,
    colorscale='RdBu',
    zmin=-1,
    zmax=1,
    hoverongaps=False
))
fig.update_layout(
    title='Interactive Correlation Matrix',
    xaxis_title="Features",
    yaxis_title="Features",
    width=800,
    height=800
)
fig.show()

# --------------------------------------------------
# 6. Threshold Optimization Visualization
# --------------------------------------------------
def plot_threshold_analysis(model, X_test, y_test):
    y_proba = model.predict_proba(X_test)[:, 1]

    # Calculate metrics for different thresholds
    thresholds = np.linspace(0, 1, 50)
    fpr_list, tpr_list, precision_list = [], [], []

    for thresh in thresholds:
        y_pred = (y_proba >= thresh).astype(int)
        y_pred_labels = model.classes_[y_pred]

        # Calculate confusion matrix
        cm = confusion_matrix(y_test, y_pred_labels, labels=model.classes_)
        tn, fp, fn, tp = cm.ravel()

        fpr = fp / (fp + tn)
        tpr = tp / (tp + fn)
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0

        fpr_list.append(fpr)
        tpr_list.append(tpr)
        precision_list.append(precision)

    # Create plot
    fig = go.Figure()

    # Add traces
    fig.add_trace(go.Scatter(
        x=thresholds, y=tpr_list,
        mode='lines',
        name='True Positive Rate (Recall)',
        line=dict(color='green')
    ))

    fig.add_trace(go.Scatter(
        x=thresholds, y=precision_list,
        mode='lines',
        name='Precision',
        line=dict(color='blue')
    ))

    fig.add_trace(go.Scatter(
        x=thresholds, y=fpr_list,
        mode='lines',
        name='False Positive Rate',
        line=dict(color='red')
    ))

    # Add optimal threshold (maximizing F1 score)
    f1_scores = 2 * (np.array(precision_list) * np.array(tpr_list)) / (
        np.array(precision_list) + np.array(tpr_list) + 1e-9)
    optimal_idx = np.argmax(f1_scores)
    optimal_threshold = thresholds[optimal_idx]

    fig.add_vline(
        x=optimal_threshold,
        line_dash="dash",
        annotation_text=f"Optimal Threshold: {optimal_threshold:.2f}",
        annotation_position="top right"
    )

    fig.update_layout(
        title='Threshold Optimization Analysis',
        xaxis_title='Threshold',
        yaxis_title='Metric Value',
        hovermode='x unified',
        width=900,
        height=500
    )

    fig.show()

plot_threshold_analysis(best_model, X_test, y_test)

# Import additional libraries needed for Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# --------------------------------------------------
# Random Forest Pipeline
# --------------------------------------------------
# Create pipeline with SMOTE for handling imbalance
rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('feature_selection', SelectKBest(score_func=f_classif)),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Hyperparameter grid for Random Forest
rf_param_grid = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 5, 10],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2],
    'classifier__max_features': ['sqrt', 'log2'],
    'feature_selection__k': [3, 'all']
}

# Create grid search for Random Forest
rf_grid_search = GridSearchCV(
    rf_pipeline,
    rf_param_grid,
    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

# Convert y_train to binary (0 and 1)
y_train_bin = y_train.map({'Fall Not Detected': 0, 'Fall Detected': 1})

# Convert y_test to binary (0 and 1) for evaluation
y_test_bin = y_test.map({'Fall Not Detected': 0, 'Fall Detected': 1})

# ... (rest of the code) ...
print("\nStarting Random Forest Grid Search...")
rf_grid_search.fit(X_train, y_train_bin)

# Get best Random Forest model
rf_best_model = rf_grid_search.best_estimator_
print("\nBest Random Forest Parameters:", rf_grid_search.best_params_)

# --------------------------------------------------
# Random Forest Evaluation
# --------------------------------------------------
print("\nEvaluating Best Random Forest Model...")
def evaluate_model(model, X_test, y_test, y_test_bin=None): # add y_test_bin=None
    # Predictions
    y_pred = model.predict(X_test)

    # Use y_test_bin for calculations if provided
    if y_test_bin is not None:
        # For metrics that require binary labels (e.g., accuracy, ROC AUC)
        y_true = y_test_bin
    else:
        y_true = y_test

    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for positive class

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred)) # use y_true here

    # ... (rest of the function, replace y_test with y_true where needed) ...

    # ROC Curve
    fpr, tpr, thresholds = roc_curve(y_true, y_proba, pos_label='Fall Detected') # use y_true
    # ...

# --------------------------------------------------
# Feature Importance for Random Forest
# --------------------------------------------------
def plot_rf_feature_importance(model, feature_names):
    """Plot feature importance for Random Forest"""
    if hasattr(model.named_steps['classifier'], 'feature_importances_'):
        importances = model.named_steps['classifier'].feature_importances_
        features = feature_names[model.named_steps['feature_selection'].get_support()]

        importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)

        plt.figure(figsize=(8, 4))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title('Random Forest Feature Importance')
        plt.show()

print("\nRandom Forest Feature Importance:")
plot_rf_feature_importance(rf_best_model, X.columns)

# --------------------------------------------------
# Final Random Forest Summary
# --------------------------------------------------
print("\n=== RANDOM FOREST TRAINING COMPLETE ===")
print(f"Best F1 Score: {rf_grid_search.best_score_:.4f}")
print("Best Parameters:")
for param, value in rf_grid_search.best_params_.items():
    print(f"- {param}: {value}")

# Compare with XGBoost
print("\n=== MODEL COMPARISON ===")
print(f"XGBoost Best F1: {grid_search.best_score_:.4f}")
print(f"Random Forest Best F1: {rf_grid_search.best_score_:.4f}")
# ... (previous code) ...

# Get test set predictions for both models
y_pred_xgb = best_model.predict(X_test)

# Convert XGBoost predictions to binary (0 and 1)
y_pred_xgb_bin = pd.Series(y_pred_xgb).map({'Fall Not Detected': 0, 'Fall Detected': 1}).values

y_pred_rf = rf_best_model.predict(X_test)

print("\nTest Set Performance:")
# Use the binary predictions for XGBoost
print(f"XGBoost Accuracy: {accuracy_score(y_test_bin, y_pred_xgb_bin):.4f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test_bin, y_pred_rf):.4f}")
print(f"XGBoost F1: {f1_score(y_test_bin, y_pred_xgb_bin):.4f}")
print(f"Random Forest F1: {f1_score(y_test_bin, y_pred_rf):.4f}")

# Set style for all plots
plt.style.use('seaborn-v0_8-whitegrid') # Changed style name to 'seaborn-v0_8-whitegrid'
plt.rcParams['figure.figsize'] = (12, 6)

# 1. Confusion Matrix with Annotations
plt.figure(figsize=(6,6))
rf_cm = confusion_matrix(y_test_bin, rf_best_model.predict(X_test))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Fall', 'Fall'],
            yticklabels=['No Fall', 'Fall'])
plt.title('Random Forest - Confusion Matrix', fontsize=14, pad=20)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()

# 2. ROC and Precision-Recall Curves Side by Side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# ROC Curve
RocCurveDisplay.from_estimator(rf_best_model, X_test, y_test_bin, ax=ax1, name='Random Forest')
ax1.plot([0, 1], [0, 1], 'k--')
ax1.set_title('ROC Curve', fontsize=14)
ax1.grid(True)

# Precision-Recall Curve
PrecisionRecallDisplay.from_estimator(rf_best_model, X_test, y_test_bin, ax=ax2, name='Random Forest')
ax2.set_title('Precision-Recall Curve', fontsize=14)
ax2.grid(True)

plt.tight_layout()
plt.show()

# 3. Feature Importance Horizontal Bar Plot
rf_feature_importances = pd.DataFrame({
    'Feature': X.columns[rf_best_model.named_steps['feature_selection'].get_support()],
    'Importance': rf_best_model.named_steps['classifier'].feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=rf_feature_importances, palette='viridis')
plt.title('Random Forest - Feature Importance', fontsize=14, pad=20)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('')
plt.grid(axis='x', alpha=0.3)
plt.show()

# 4. Class Prediction Distribution
rf_probs = rf_best_model.predict_proba(X_test)[:, 1]
plt.figure(figsize=(10, 6))
sns.histplot(x=rf_probs, hue=y_test_bin, bins=30, kde=True,
             palette={0:'skyblue', 1:'coral'}, alpha=0.6)
plt.title('Random Forest - Prediction Probability Distribution', fontsize=14, pad=20)
plt.xlabel('Predicted Probability of Fall', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend(title='Actual Class', labels=['No Fall', 'Fall'])
plt.grid(alpha=0.3)
plt.show()

# 5. Performance Metrics Comparison (Table)
from tabulate import tabulate

rf_metrics = classification_report(y_test_bin, rf_best_model.predict(X_test), output_dict=True)
metrics_data = [
    ["Accuracy", rf_metrics['accuracy']],
    ["Precision (Fall)", rf_metrics['1']['precision']],
    ["Recall (Fall)", rf_metrics['1']['recall']],
    ["F1-Score (Fall)", rf_metrics['1']['f1-score']],
    ["ROC AUC", roc_auc_score(y_test_bin, rf_probs)]
]

print("\n\033[1mRandom Forest Performance Metrics:\033[0m")
print(tabulate(metrics_data, headers=["Metric", "Score"], floatfmt=".4f", tablefmt="grid"))

# 6. Decision Threshold Analysis (Interactive)
from ipywidgets import interact, FloatSlider

@interact(threshold=FloatSlider(min=0, max=1, step=0.05, value=0.5, description='Threshold:'))
def threshold_analysis(threshold):
    y_pred = (rf_probs >= threshold).astype(int)
    cm = confusion_matrix(y_test_bin, y_pred)

    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Fall', 'Fall'],
                yticklabels=['No Fall', 'Fall'])
    plt.title(f'Confusion Matrix @ {threshold:.2f}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    plt.subplot(1, 2, 2)
    report = classification_report(y_test_bin, y_pred, output_dict=True)
    metrics = ['precision', 'recall', 'f1-score']
    values = [report['1']['precision'], report['1']['recall'], report['1']['f1-score']]
    sns.barplot(x=metrics, y=values, palette='rocket')
    plt.ylim(0, 1)
    plt.title('Performance Metrics')

    plt.tight_layout()
    plt.show()

  # Import additional libraries needed for Random Forest
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score

# --------------------------------------------------
# Random Forest Pipeline
# --------------------------------------------------
# Create pipeline with SMOTE for handling imbalance
rf_pipeline = Pipeline([
    ('scaler', StandardScaler()),
    ('smote', SMOTE(random_state=42)),
    ('feature_selection', SelectKBest(score_func=f_classif)),
    ('classifier', RandomForestClassifier(random_state=42))
])

# Hyperparameter grid for Random Forest
rf_param_grid = {
    'classifier__n_estimators': [100, 200],
    'classifier__max_depth': [None, 5, 10],
    'classifier__min_samples_split': [2, 5],
    'classifier__min_samples_leaf': [1, 2],
    'classifier__max_features': ['sqrt', 'log2'],
    'feature_selection__k': [3, 'all']
}

# Create grid search for Random Forest
rf_grid_search = GridSearchCV(
    rf_pipeline,
    rf_param_grid,
    cv=StratifiedKFold(n_splits=3, shuffle=True, random_state=42),
    scoring='f1',
    n_jobs=-1,
    verbose=1
)

# Convert y_train to binary (0 and 1)
y_train_bin = y_train.map({'Fall Not Detected': 0, 'Fall Detected': 1})

# Convert y_test to binary (0 and 1) for evaluation
y_test_bin = y_test.map({'Fall Not Detected': 0, 'Fall Detected': 1})

# ... (rest of the code) ...
print("\nStarting Random Forest Grid Search...")
rf_grid_search.fit(X_train, y_train_bin)

# Get best Random Forest model
rf_best_model = rf_grid_search.best_estimator_
print("\nBest Random Forest Parameters:", rf_grid_search.best_params_)

# --------------------------------------------------
# Random Forest Evaluation
# --------------------------------------------------
print("\nEvaluating Best Random Forest Model...")
def evaluate_model(model, X_test, y_test, y_test_bin=None): # add y_test_bin=None
    # Predictions
    y_pred = model.predict(X_test)

    # Use y_test_bin for calculations if provided
    if y_test_bin is not None:
        # For metrics that require binary labels (e.g., accuracy, ROC AUC)
        y_true = y_test_bin
    else:
        y_true = y_test

    y_proba = model.predict_proba(X_test)[:, 1]  # Probabilities for positive class

    # Classification Report
    print("\nClassification Report:")
    print(classification_report(y_true, y_pred)) # use y_true here

    # ... (rest of the function, replace y_test with y_true where needed) ...

    # ROC Curve
    fpr, tpr, thresholds = roc_curve(y_true, y_proba, pos_label='Fall Detected') # use y_true
    # ...

# --------------------------------------------------
# Feature Importance for Random Forest
# --------------------------------------------------
def plot_rf_feature_importance(model, feature_names):
    """Plot feature importance for Random Forest"""
    if hasattr(model.named_steps['classifier'], 'feature_importances_'):
        importances = model.named_steps['classifier'].feature_importances_
        features = feature_names[model.named_steps['feature_selection'].get_support()]

        importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})
        importance_df = importance_df.sort_values('Importance', ascending=False)

        plt.figure(figsize=(8, 4))
        sns.barplot(x='Importance', y='Feature', data=importance_df)
        plt.title('Random Forest Feature Importance')
        plt.show()

print("\nRandom Forest Feature Importance:")
plot_rf_feature_importance(rf_best_model, X.columns)

# --------------------------------------------------
# Final Random Forest Summary
# --------------------------------------------------
print("\n=== RANDOM FOREST TRAINING COMPLETE ===")
print(f"Best F1 Score: {rf_grid_search.best_score_:.4f}")
print("Best Parameters:")
for param, value in rf_grid_search.best_params_.items():
    print(f"- {param}: {value}")

# Compare with XGBoost
print("\n=== MODEL COMPARISON ===")
print(f"XGBoost Best F1: {grid_search.best_score_:.4f}")
print(f"Random Forest Best F1: {rf_grid_search.best_score_:.4f}")
# ... (previous code) ...

# Get test set predictions for both models
y_pred_xgb = best_model.predict(X_test)

# Convert XGBoost predictions to binary (0 and 1)
y_pred_xgb_bin = pd.Series(y_pred_xgb).map({'Fall Not Detected': 0, 'Fall Detected': 1}).values

y_pred_rf = rf_best_model.predict(X_test)

print("\nTest Set Performance:")
# Use the binary predictions for XGBoost
print(f"XGBoost Accuracy: {accuracy_score(y_test_bin, y_pred_xgb_bin):.4f}")
print(f"Random Forest Accuracy: {accuracy_score(y_test_bin, y_pred_rf):.4f}")
print(f"XGBoost F1: {f1_score(y_test_bin, y_pred_xgb_bin):.4f}")
print(f"Random Forest F1: {f1_score(y_test_bin, y_pred_rf):.4f}")

# Set style for all plots
plt.style.use('seaborn-v0_8-whitegrid') # Changed style name to 'seaborn-v0_8-whitegrid'
plt.rcParams['figure.figsize'] = (12, 6)

# 1. Confusion Matrix with Annotations
plt.figure(figsize=(6,6))
rf_cm = confusion_matrix(y_test_bin, rf_best_model.predict(X_test))
sns.heatmap(rf_cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=['No Fall', 'Fall'],
            yticklabels=['No Fall', 'Fall'])
plt.title('Random Forest - Confusion Matrix', fontsize=14, pad=20)
plt.xlabel('Predicted Label', fontsize=12)
plt.ylabel('True Label', fontsize=12)
plt.show()

# 2. ROC and Precision-Recall Curves Side by Side
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

# ROC Curve
RocCurveDisplay.from_estimator(rf_best_model, X_test, y_test_bin, ax=ax1, name='Random Forest')
ax1.plot([0, 1], [0, 1], 'k--')
ax1.set_title('ROC Curve', fontsize=14)
ax1.grid(True)

# Precision-Recall Curve
PrecisionRecallDisplay.from_estimator(rf_best_model, X_test, y_test_bin, ax=ax2, name='Random Forest')
ax2.set_title('Precision-Recall Curve', fontsize=14)
ax2.grid(True)

plt.tight_layout()
plt.show()

# 3. Feature Importance Horizontal Bar Plot
rf_feature_importances = pd.DataFrame({
    'Feature': X.columns[rf_best_model.named_steps['feature_selection'].get_support()],
    'Importance': rf_best_model.named_steps['classifier'].feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=rf_feature_importances, palette='viridis')
plt.title('Random Forest - Feature Importance', fontsize=14, pad=20)
plt.xlabel('Importance Score', fontsize=12)
plt.ylabel('')
plt.grid(axis='x', alpha=0.3)
plt.show()

# 4. Class Prediction Distribution
rf_probs = rf_best_model.predict_proba(X_test)[:, 1]
plt.figure(figsize=(10, 6))
sns.histplot(x=rf_probs, hue=y_test_bin, bins=30, kde=True,
             palette={0:'skyblue', 1:'coral'}, alpha=0.6)
plt.title('Random Forest - Prediction Probability Distribution', fontsize=14, pad=20)
plt.xlabel('Predicted Probability of Fall', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.legend(title='Actual Class', labels=['No Fall', 'Fall'])
plt.grid(alpha=0.3)
plt.show()

# 5. Performance Metrics Comparison (Table)
from tabulate import tabulate

rf_metrics = classification_report(y_test_bin, rf_best_model.predict(X_test), output_dict=True)
metrics_data = [
    ["Accuracy", rf_metrics['accuracy']],
    ["Precision (Fall)", rf_metrics['1']['precision']],
    ["Recall (Fall)", rf_metrics['1']['recall']],
    ["F1-Score (Fall)", rf_metrics['1']['f1-score']],
    ["ROC AUC", roc_auc_score(y_test_bin, rf_probs)]
]

print("\n\033[1mRandom Forest Performance Metrics:\033[0m")
print(tabulate(metrics_data, headers=["Metric", "Score"], floatfmt=".4f", tablefmt="grid"))

# 6. Decision Threshold Analysis (Interactive)
from ipywidgets import interact, FloatSlider

@interact(threshold=FloatSlider(min=0, max=1, step=0.05, value=0.5, description='Threshold:'))
def threshold_analysis(threshold):
    y_pred = (rf_probs >= threshold).astype(int)
    cm = confusion_matrix(y_test_bin, y_pred)

    plt.figure(figsize=(12, 4))

    plt.subplot(1, 2, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['No Fall', 'Fall'],
                yticklabels=['No Fall', 'Fall'])
    plt.title(f'Confusion Matrix @ {threshold:.2f}')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')

    plt.subplot(1, 2, 2)
    report = classification_report(y_test_bin, y_pred, output_dict=True)
    metrics = ['precision', 'recall', 'f1-score']
    values = [report['1']['precision'], report['1']['recall'], report['1']['f1-score']]
    sns.barplot(x=metrics, y=values, palette='rocket')
    plt.ylim(0, 1)
    plt.title('Performance Metrics')

    plt.tight_layout()
    plt.show()